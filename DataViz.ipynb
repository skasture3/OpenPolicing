{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1OeBOPzUGHh9ZEu7Va2JbY-_63ZeV_VuQ","authorship_tag":"ABX9TyP8ujAyZHTKvtwFZBYMsnIk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"z259eGUUhmPl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732385766938,"user_tz":300,"elapsed":8621,"user":{"displayName":"DataViz","userId":"07137367932971846404"}},"outputId":"bfe092f1-b271-4ec7-9248-33a25efc1591"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!head -n 10 '/content/drive/My Drive/StateData/wi.csv'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JMT502RCMsH6","executionInfo":{"status":"ok","timestamp":1732385434430,"user_tz":300,"elapsed":283,"user":{"displayName":"DataViz","userId":"07137367932971846404"}},"outputId":"933bc890-6f3d-428a-ff0e-d10fa1cbb1ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["head: cannot open '/content/drive/My Drive/StateData/wi.csv' for reading: No such file or directory\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# Initialize variables for aggregation\n","traffic_stops_per_date = {}\n","\n","# Define chunk size for processing\n","chunk_size = 100000\n","file_path = \"/content/drive/My Drive/StateData/wy.csv\"  # Adjust the file path if necessary\n","\n","# Process the file in chunks\n","for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n","    # Convert the 'date' column to datetime\n","    chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n","\n","    # Drop rows with invalid dates\n","    chunk = chunk.dropna(subset=['date'])\n","\n","    # Group by date and count traffic stops\n","    chunk_counts = chunk.groupby('date').size()\n","\n","    # Aggregate the results\n","    for date, count in chunk_counts.items():\n","        if date in traffic_stops_per_date:\n","            traffic_stops_per_date[date] += count\n","        else:\n","            traffic_stops_per_date[date] = count\n","\n","# Convert the aggregated dictionary to a DataFrame\n","result = pd.DataFrame(list(traffic_stops_per_date.items()), columns=['date', 'traffic_stop_count'])\n","\n","# Add the state column\n","result['state'] = 'WY'  # Vermont\n","\n","# Save the processed data\n","output_path = \"/content/drive/My Drive/StateData/processed_wy.csv\"  # Adjust the output file path if necessary\n","result.to_csv(output_path, index=False)\n","\n","# Display the processed data\n","print(result.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"U9M-DEzzMHL3","executionInfo":{"status":"error","timestamp":1732385435790,"user_tz":300,"elapsed":711,"user":{"displayName":"DataViz","userId":"07137367932971846404"}},"outputId":"807d9857-df85-4012-bc1d-a84ef038589b"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/My Drive/StateData/wy.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-3e2d71af63f5>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Process the file in chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Convert the 'date' column to datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/StateData/wy.csv'"]}]},{"cell_type":"code","source":["print(result.tail())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"Df-7zjetkNui","executionInfo":{"status":"error","timestamp":1732385451407,"user_tz":300,"elapsed":284,"user":{"displayName":"DataViz","userId":"07137367932971846404"}},"outputId":"7acba05c-3ab0-49fb-9b3d-b8729b80de24"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'result' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-d189546e5e7b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"]}]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","# Step 1: Combine all CSVs\n","directory = \"/content/drive/My Drive/StateData/\"  # Directory where all processed CSVs are stored\n","all_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.startswith(\"processed_\")]\n","\n","# Read and combine all files\n","df_list = [pd.read_csv(file) for file in all_files]\n","data = pd.concat(df_list, ignore_index=True)\n","\n","# Ensure 'date' column is in datetime format\n","data['date'] = pd.to_datetime(data['date'])\n","\n","# Step 2: Group by state and date, calculate cumulative traffic stops\n","data = data.sort_values(by=['state', 'date'])  # Sort by state and date\n","data['cumulative_traffic_stops'] = data.groupby('state')['traffic_stop_count'].cumsum()\n","\n","# Save as CSV\n","output_csv_path = \"/content/drive/My Drive/StateData/combined_traffic_data.csv\"\n","data.to_csv(output_csv_path, index=False)\n","\n","# Verify saved file\n","print(f\"Data saved to {output_csv_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t5FtcZKvt0b4","executionInfo":{"status":"ok","timestamp":1732385476717,"user_tz":300,"elapsed":22916,"user":{"displayName":"DataViz","userId":"07137367932971846404"}},"outputId":"9b8e2081-2b73-4b34-dcdb-a5d4d63014c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data saved to /content/drive/My Drive/StateData/combined_traffic_data.csv\n"]}]},{"cell_type":"code","source":["# Group by state and count the number of rows per state\n","rows_per_state = data.groupby('state').size()\n","\n","# Display the result\n","print(rows_per_state)\n","\n","\n","# Convert to a DataFrame and sort by the count\n","rows_per_state_df = rows_per_state.reset_index(name='row_count').sort_values(by='row_count', ascending=False)\n","\n","# Display the result\n","print(rows_per_state_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"inJs_Majui_B","executionInfo":{"status":"ok","timestamp":1732385476717,"user_tz":300,"elapsed":3,"user":{"displayName":"DataViz","userId":"07137367932971846404"}},"outputId":"822c50b3-d797-4f21-f73f-6243e6f6011c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["state\n","CA    2545\n","CO    2921\n","CT     731\n","GA    1827\n","IA    3768\n","IL    2192\n","MA    3287\n","MD     459\n","MO       6\n","MS    1297\n","MT    3207\n","NC    5844\n","ND    2002\n","NE      60\n","NH     730\n","NJ    2922\n","NV    1324\n","NY    2905\n","OH    2559\n","OR       5\n","RI    3809\n","SC    4228\n","SD    1308\n","TN    5769\n","VA     538\n","VT    2010\n","WI    2072\n","WY     731\n","dtype: int64\n","   state  row_count\n","11    NC       5844\n","23    TN       5769\n","21    SC       4228\n","20    RI       3809\n","4     IA       3768\n","6     MA       3287\n","10    MT       3207\n","15    NJ       2922\n","1     CO       2921\n","17    NY       2905\n","18    OH       2559\n","0     CA       2545\n","5     IL       2192\n","26    WI       2072\n","25    VT       2010\n","12    ND       2002\n","3     GA       1827\n","16    NV       1324\n","22    SD       1308\n","9     MS       1297\n","27    WY        731\n","2     CT        731\n","14    NH        730\n","24    VA        538\n","7     MD        459\n","13    NE         60\n","8     MO          6\n","19    OR          5\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset\n","data_path = \"/content/drive/My Drive/StateData/combined_traffic_data.parquet\"\n","data = pd.read_parquet(data_path)\n","\n","# Ensure 'date' is in datetime format\n","data['date'] = pd.to_datetime(data['date'])\n","\n","# Filter out dates before February 20, 2004\n","start_date = pd.Timestamp(\"2004-02-20\")\n","filtered_data = data[data['date'] >= start_date]\n","\n","# Add a 'week' column (start of the week)\n","filtered_data['week'] = filtered_data['date'].dt.to_period('W').apply(lambda r: r.start_time)\n","\n","# Group by state and week, and sum traffic stops for each week\n","weekly_data = filtered_data.groupby(['state', 'week'], as_index=False).agg({\n","    'traffic_stop_count': 'sum',\n","    'cumulative_traffic_stops': 'max'  # Use the max cumulative value for each week\n","})\n","\n","# Save the weekly aggregated data to both CSV and Parquet files\n","output_csv_path = \"/content/drive/My Drive/StateData/weekly_traffic_data.csv\"\n","output_parquet_path = \"/content/drive/My Drive/StateData/weekly_traffic_data.parquet\"\n","\n","# Save as CSV\n","weekly_data.to_csv(output_csv_path, index=False)\n","\n","# Save as Parquet\n","weekly_data.to_parquet(output_parquet_path, index=False)\n","\n","print(f\"Weekly aggregated data saved to CSV at {output_csv_path}\")\n","print(f\"Weekly aggregated data saved to Parquet at {output_parquet_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kEFGvadbbtjK","executionInfo":{"status":"ok","timestamp":1732385819256,"user_tz":300,"elapsed":8158,"user":{"displayName":"DataViz","userId":"07137367932971846404"}},"outputId":"5382a864-8377-4d56-f5ad-54c942686a36"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-30deaa9bd9f3>:15: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  filtered_data['week'] = filtered_data['date'].dt.to_period('W').apply(lambda r: r.start_time)\n"]},{"output_type":"stream","name":"stdout","text":["Weekly aggregated data saved to CSV at /content/drive/My Drive/StateData/weekly_traffic_data.csv\n","Weekly aggregated data saved to Parquet at /content/drive/My Drive/StateData/weekly_traffic_data.parquet\n"]}]},{"cell_type":"code","source":["pip install dash"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tnAjTRExYOhi","executionInfo":{"status":"ok","timestamp":1732385832935,"user_tz":300,"elapsed":7155,"user":{"displayName":"DataViz","userId":"07137367932971846404"}},"outputId":"b8b7db08-f8d2-45ed-e2b5-f438a8a63b0e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting dash\n","  Downloading dash-2.18.2-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash) (3.0.3)\n","Collecting Werkzeug<3.1 (from dash)\n","  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash) (5.24.1)\n","Collecting dash-html-components==2.0.0 (from dash)\n","  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n","Collecting dash-core-components==2.0.0 (from dash)\n","  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting dash-table==5.0.0 (from dash)\n","  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash) (8.5.0)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash) (4.12.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dash) (2.32.3)\n","Collecting retrying (from dash)\n","  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash) (1.6.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash) (75.1.0)\n","Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash) (3.1.4)\n","Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash) (8.1.7)\n","Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash) (1.9.0)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash) (9.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash) (24.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from Werkzeug<3.1->dash) (3.0.2)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash) (3.21.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dash) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dash) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dash) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dash) (2024.8.30)\n","Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying->dash) (1.16.0)\n","Downloading dash-2.18.2-py3-none-any.whl (7.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n","Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n","Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n","Downloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n","Installing collected packages: dash-table, dash-html-components, dash-core-components, Werkzeug, retrying, dash\n","  Attempting uninstall: Werkzeug\n","    Found existing installation: Werkzeug 3.1.3\n","    Uninstalling Werkzeug-3.1.3:\n","      Successfully uninstalled Werkzeug-3.1.3\n","Successfully installed Werkzeug-3.0.6 dash-2.18.2 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 retrying-1.3.4\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import plotly.express as px\n","from dash import Dash, dcc, html, Input, Output\n","\n","# Load the weekly aggregated data from the .parquet file\n","data_path = \"/content/drive/My Drive/StateData/weekly_traffic_data.parquet\"\n","data = pd.read_parquet(data_path)\n","\n","# Ensure 'week' column is in datetime format\n","data['week'] = pd.to_datetime(data['week'])\n","\n","# Calculate the total number of weeks\n","total_weeks = len(data['week'].unique())\n","\n","# Initialize the Dash app\n","app = Dash(__name__)\n","\n","# App layout\n","app.layout = html.Div([\n","    html.H1(\"Interactive Weekly Cumulative Traffic Stops Map\"),\n","    dcc.Graph(id=\"choropleth-map\"),\n","    dcc.Slider(\n","        id=\"week-slider\",\n","        min=0,\n","        max=total_weeks - 1,\n","        step=1,\n","        value=total_weeks - 1,  # Start from the most recent week\n","        tooltip={\"placement\": \"bottom\", \"always_visible\": False},  # Disable default tooltip\n","        marks={},  # Remove all marks\n","        updatemode='drag'\n","    ),\n","    html.Div(id=\"slider-label\", style={\"textAlign\": \"center\", \"marginTop\": \"20px\", \"fontSize\": \"18px\"})\n","])\n","\n","# Callback to update the slider label dynamically\n","@app.callback(\n","    Output(\"slider-label\", \"children\"),\n","    [Input(\"week-slider\", \"value\")]\n",")\n","def update_slider_label(selected_week_index):\n","    # Calculate \"years and weeks ago\"\n","    weeks_ago = total_weeks - selected_week_index - 1\n","    years = weeks_ago // 52\n","    weeks = weeks_ago % 52\n","\n","    if weeks_ago == 0:\n","        return \"This week\"\n","    elif years > 0:\n","        return f\"{years} year{'s' if years > 1 else ''} and {weeks} week{'s' if weeks > 1 else ''} ago\"\n","    else:\n","        return f\"{weeks} week{'s' if weeks > 1 else ''} ago\"\n","\n","# Callback to update the map based on the selected week\n","@app.callback(\n","    Output(\"choropleth-map\", \"figure\"),\n","    [Input(\"week-slider\", \"value\")]\n",")\n","def update_map(selected_week_index):\n","    # Filter the data up to the selected week\n","    selected_week = sorted(data['week'].unique())[selected_week_index]\n","    filtered_data = data[data['week'] <= selected_week]\n","\n","    # Take the latest cumulative traffic stops for each state up to the selected week\n","    filtered_data = filtered_data.groupby('state').last().reset_index()\n","\n","    # Create the choropleth map with a more drastic color scheme\n","    fig = px.choropleth(\n","        filtered_data,\n","        locations=\"state\",\n","        locationmode=\"USA-states\",\n","        color=\"cumulative_traffic_stops\",\n","        hover_name=\"state\",\n","        scope=\"usa\",\n","        title=f\"Cumulative Traffic Stops up to {selected_week.strftime('%Y-%m-%d')}\",\n","        color_continuous_scale=\"Plasma\"  # Use a more vibrant color scheme\n","    )\n","    return fig\n","\n","# Run the app\n","if __name__ == \"__main__\":\n","    app.run_server(debug=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":672},"id":"2tC6z8Y0YHEE","executionInfo":{"status":"ok","timestamp":1732385833846,"user_tz":300,"elapsed":913,"user":{"displayName":"DataViz","userId":"07137367932971846404"}},"outputId":"98df3fd7-52ff-4160-9afc-e46005758b46"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["(async (port, path, width, height, cache, element) => {\n","    if (!google.colab.kernel.accessAllowed && !cache) {\n","      return;\n","    }\n","    element.appendChild(document.createTextNode(''));\n","    const url = await google.colab.kernel.proxyPort(port, {cache});\n","    const iframe = document.createElement('iframe');\n","    iframe.src = new URL(path, url).toString();\n","    iframe.height = height;\n","    iframe.width = width;\n","    iframe.style.border = 0;\n","    iframe.allow = [\n","        'accelerometer',\n","        'autoplay',\n","        'camera',\n","        'clipboard-read',\n","        'clipboard-write',\n","        'gyroscope',\n","        'magnetometer',\n","        'microphone',\n","        'serial',\n","        'usb',\n","        'xr-spatial-tracking',\n","    ].join('; ');\n","    element.appendChild(iframe);\n","  })(8050, \"/\", \"100%\", 650, false, window.element)"]},"metadata":{}}]},{"cell_type":"code","source":["import pandas as pd\n","import plotly.express as px\n","from dash import Dash, dcc, html, Input, Output, callback_context\n","from datetime import datetime\n","import dash\n","\n","# Initialize the Dash app\n","app = Dash(__name__)\n","\n","# Load the weekly aggregated data from the .parquet file\n","data_path = \"/content/drive/My Drive/StateData/weekly_traffic_data.parquet\"\n","data = pd.read_parquet(data_path)\n","\n","# Ensure 'week' column is in datetime format and sorted\n","data['week'] = pd.to_datetime(data['week'])\n","data = data.sort_values('week').reset_index(drop=True)\n","\n","# Get sorted unique weeks as a list\n","unique_weeks = sorted(data['week'].unique())\n","total_weeks = len(unique_weeks)\n","\n","# Function to find the index of the start date\n","def find_start_index(start_date):\n","    if start_date in unique_weeks:\n","        return unique_weeks.index(start_date)\n","    else:\n","        # Find the next available week after the start_date\n","        for i, week in enumerate(unique_weeks):\n","            if week > start_date:\n","                return i\n","        # If no week is found after start_date, return the last index\n","        return total_weeks - 1\n","\n","# App layout\n","app.layout = html.Div([\n","    html.H1(\"Interactive Weekly Cumulative Traffic Stops Map\"),\n","\n","    # Date Picker for Start Date\n","    html.Div([\n","        html.Label(\"Select Start Date:\"),\n","        dcc.DatePickerSingle(\n","            id='start-date-picker',\n","            min_date_allowed=min(unique_weeks),  # Use min() instead of unique_weeks.min()\n","            max_date_allowed=max(unique_weeks),  # Use max() instead of unique_weeks.max()\n","            initial_visible_month=min(unique_weeks),  # Use min() instead of unique_weeks.min()\n","            date=min(unique_weeks).date()  # Use min() instead of unique_weeks.min()\n","        )\n","    ], style={\"marginBottom\": \"20px\"}),\n","\n","    dcc.Graph(id=\"choropleth-map\"),\n","\n","    # Slider for Weeks since Start Date\n","    dcc.Slider(\n","        id=\"week-slider\",\n","        min=0,\n","        max=total_weeks - 1,\n","        step=1,\n","        value=total_weeks - 1,  # Start from the most recent week\n","        tooltip={\"placement\": \"bottom\", \"always_visible\": False},\n","        marks={},  # Marks will be dynamically generated\n","        updatemode='drag'\n","    ),\n","\n","    html.Div(id=\"slider-label\", style={\"textAlign\": \"center\", \"marginTop\": \"20px\", \"fontSize\": \"18px\"})\n","])\n","\n","# Callback to update the slider based on the selected start date\n","@app.callback(\n","    [Output(\"week-slider\", \"min\"),\n","     Output(\"week-slider\", \"max\"),\n","     Output(\"week-slider\", \"marks\"),\n","     Output(\"week-slider\", \"value\")],\n","    [Input(\"start-date-picker\", \"date\")]\n",")\n","def update_slider(start_date):\n","    if start_date is None:\n","        # If no date is selected, set to default min date\n","        default_start_date = min(unique_weeks)\n","    else:\n","        try:\n","            # Convert start_date to datetime\n","            start_date = pd.to_datetime(start_date)\n","            default_start_date = start_date\n","        except Exception:\n","            # If invalid date, set to default\n","            default_start_date = min(unique_weeks)\n","\n","    try:\n","        # Find the index of the start_date in unique_weeks\n","        start_index = find_start_index(default_start_date)\n","\n","        # Calculate the number of weeks from start_date to the present\n","        max_index = total_weeks - 1\n","        num_weeks = max_index - start_index + 1\n","\n","        if num_weeks <= 0:\n","            # If start_index is beyond max, set to min and max as zero\n","            return 0, 0, {0: unique_weeks[max_index].strftime('%Y-%m-%d')}, 0\n","\n","        # Generate marks every 10 weeks for readability\n","        marks = {}\n","        interval = max(1, num_weeks // 10)\n","        for i in range(start_index, max_index + 1, interval):\n","            week_label = unique_weeks[i].strftime('%Y-%m-%d')\n","            marks[i - start_index] = week_label\n","\n","        # Ensure the last week is always marked\n","        if (max_index - start_index) % interval != 0:\n","            marks[num_weeks - 1] = unique_weeks[max_index].strftime('%Y-%m-%d')\n","\n","        # Set the initial slider value to the latest week in the selected range\n","        slider_value = num_weeks - 1\n","\n","        return 0, num_weeks - 1, marks, slider_value\n","    except Exception as e:\n","        # Log the exception and prevent the callback from updating outputs\n","        print(f\"Error in update_slider: {e}\")\n","        return dash.no_update, dash.no_update, dash.no_update, dash.no_update\n","\n","# Callback to update the slider label based on the selected week\n","@app.callback(\n","    Output(\"slider-label\", \"children\"),\n","    [Input(\"week-slider\", \"value\"),\n","     Input(\"start-date-picker\", \"date\")]\n",")\n","def update_slider_label(selected_week_offset, start_date):\n","    if start_date is None:\n","        # If no date is selected, use the default min date\n","        default_start_date = min(unique_weeks)\n","    else:\n","        try:\n","            # Convert start_date to datetime\n","            default_start_date = pd.to_datetime(start_date)\n","        except Exception:\n","            # If invalid date, use the default min date\n","            default_start_date = min(unique_weeks)\n","\n","    try:\n","        # Find the index of the start_date in unique_weeks\n","        start_index = find_start_index(default_start_date)\n","\n","        # Calculate the selected week index\n","        selected_week_index = start_index + selected_week_offset\n","        selected_week_index = min(selected_week_index, total_weeks - 1)  # Ensure it doesn't exceed\n","\n","        selected_week = unique_weeks[selected_week_index]\n","\n","        # Calculate \"years and weeks ago\" from the selected week to the present\n","        weeks_ago = total_weeks - (selected_week_index + 1)\n","        years = weeks_ago // 52\n","        weeks = weeks_ago % 52\n","\n","        if weeks_ago == 0:\n","            return \"This week\"\n","        elif years > 0:\n","            return f\"{years} year{'s' if years > 1 else ''} and {weeks} week{'s' if weeks > 1 else ''} ago\"\n","        else:\n","            return f\"{weeks} week{'s' if weeks > 1 else ''} ago\"\n","    except Exception as e:\n","        # Log the exception and prevent the callback from updating the output\n","        print(f\"Error in update_slider_label: {e}\")\n","        return \"Error updating label\"\n","\n","@app.callback(\n","    Output(\"choropleth-map\", \"figure\"),\n","    [Input(\"week-slider\", \"value\"),\n","     Input(\"start-date-picker\", \"date\")]\n",")\n","def update_map(selected_week_offset, start_date):\n","    if start_date is None:\n","        # If no date is selected, use the default min date\n","        default_start_date = min(unique_weeks)\n","    else:\n","        try:\n","            # Convert start_date to datetime\n","            default_start_date = pd.to_datetime(start_date)\n","        except Exception:\n","            # If invalid date, use the default min date\n","            default_start_date = min(unique_weeks)\n","\n","    try:\n","        # Find the index of the start_date in unique_weeks\n","        start_index = find_start_index(default_start_date)\n","\n","        # Calculate the selected week index\n","        selected_week_index = start_index + selected_week_offset\n","        selected_week_index = min(selected_week_index, total_weeks - 1)  # Ensure it doesn't exceed\n","\n","        selected_week = unique_weeks[selected_week_index]\n","\n","        # Filter the data up to the selected week\n","        filtered_data = data[data['week'] <= selected_week]\n","\n","        # Take the latest cumulative traffic stops for each state up to the selected week\n","        filtered_data = filtered_data.groupby('state').last().reset_index()\n","\n","        # Create a custom hover text column\n","        filtered_data['hover_text'] = (\n","            \"State: \" + filtered_data['state'] +\n","            \"<br>Cumulative Traffic Stops: \" + filtered_data['cumulative_traffic_stops'].apply(lambda x: f\"{x:,.0f}\")\n","        )\n","\n","        # Create the choropleth map with custom hover text\n","        fig = px.choropleth(\n","            filtered_data,\n","            locations=\"state\",\n","            locationmode=\"USA-states\",\n","            color=\"cumulative_traffic_stops\",\n","            hover_name=\"state\",\n","            scope=\"usa\",\n","            title=f\"Cumulative Traffic Stops up to {selected_week.strftime('%Y-%m-%d')}\",\n","            color_continuous_scale=\"Plasma\",\n","            hover_data={'hover_text': True},  # Use only custom hover text\n","        )\n","\n","        # Update the layout to only show the custom hover text\n","        fig.update_traces(hovertemplate='%{customdata[0]}<extra></extra>')\n","        fig.update_layout(transition_duration=500)\n","        return fig\n","    except Exception as e:\n","        # Log the exception and return an empty figure with an error message\n","        print(f\"Error in update_map: {e}\")\n","        return {\n","            \"data\": [],\n","            \"layout\": {\n","                \"title\": \"Error updating the map.\",\n","                \"xaxis\": {\"visible\": False},\n","                \"yaxis\": {\"visible\": False},\n","            }\n","        }\n","\n","# Run the app\n","if __name__ == \"__main__\":\n","    app.run_server(debug=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":672},"id":"LEqWIVfXh4Gk","executionInfo":{"status":"ok","timestamp":1732385878332,"user_tz":300,"elapsed":357,"user":{"displayName":"DataViz","userId":"07137367932971846404"}},"outputId":"a45bb090-d8eb-4917-bff9-5be3f800d6e5"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["(async (port, path, width, height, cache, element) => {\n","    if (!google.colab.kernel.accessAllowed && !cache) {\n","      return;\n","    }\n","    element.appendChild(document.createTextNode(''));\n","    const url = await google.colab.kernel.proxyPort(port, {cache});\n","    const iframe = document.createElement('iframe');\n","    iframe.src = new URL(path, url).toString();\n","    iframe.height = height;\n","    iframe.width = width;\n","    iframe.style.border = 0;\n","    iframe.allow = [\n","        'accelerometer',\n","        'autoplay',\n","        'camera',\n","        'clipboard-read',\n","        'clipboard-write',\n","        'gyroscope',\n","        'magnetometer',\n","        'microphone',\n","        'serial',\n","        'usb',\n","        'xr-spatial-tracking',\n","    ].join('; ');\n","    element.appendChild(iframe);\n","  })(8050, \"/\", \"100%\", 650, false, window.element)"]},"metadata":{}}]}]}